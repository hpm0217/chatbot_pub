import streamlit as st
import base64

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage

from dotenv import load_dotenv
import os

#load_dotenv()

# --- í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜ ---
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

from pdfminer.high_level import extract_text

def get_pdf_text(filename):
    raw_text = extract_text(filename)
    return raw_text

def process_uploaded_file(uploaded_file):
    # Load document if file is uploaded
    if uploaded_file is not None:
        # loader
        raw_text = get_pdf_text(uploaded_file)
        if not raw_text or len(raw_text.strip()) == 0:
            st.error("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ğŸ¾")
            return None, None

        # splitter
        text_splitter = CharacterTextSplitter(
            separator = "\n\n",
            chunk_size = 1000,
            chunk_overlap  = 200,
        )
        all_splits = text_splitter.create_documents([raw_text])
        if not all_splits or len(all_splits) == 0:
            st.error("PDFì—ì„œ ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤! ğŸ¾")
            return None, None

        print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")

        # storage
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
        return vectorstore, raw_text
    return None, None

def generate_response(query_text, vectorstore, callback):
    # retriever 
    docs_list = vectorstore.similarity_search(query_text, k=3)
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"'ë¬¸ì„œ{i+1}':{doc.page_content}\n"
    # generator
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback])
    # chaining
    rag_prompt = [
        SystemMessage(
            content="ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” 'ë¬¸ì„œë´‡'ì´ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ ë§ˆ."
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸:{query_text}\n\n{docs}"
        ),
    ]
    response = llm(rag_prompt)
    return response.content

def generate_summarize(callback):
    PET_DOC_PATH = os.path.join(os.path.dirname(__file__), "pet_doc.pdf")
    if not os.path.exists(PET_DOC_PATH):
        return "pet_doc.pdf íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ì—†ìŠµë‹ˆë‹¤! ğŸ¾"
    raw_text = get_pdf_text(PET_DOC_PATH)
    if not raw_text or len(raw_text.strip()) == 0:
        return "pet_doc.pdfì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback])
    rag_prompt = [
        SystemMessage(
            content="ë‹¤ìŒ ë‚˜ì˜¬ ë¬¸ì„œë¥¼ 'Notion style'ë¡œ ìš”ì•½í•´ì¤˜. ì¤‘ìš”í•œ ë‚´ìš©ë§Œ."
        ),
        HumanMessage(
            content=raw_text
        ),
    ]
    response = llm(rag_prompt)
    return response.content

# --- UI ë° ìƒíƒœ ê´€ë¦¬ ì½”ë“œ ---
# ê·€ì—¬ìš´ ë™ë¬¼ ì´ëª¨ì§€ì™€ ìƒ‰ìƒ í…Œë§ˆ
ANIMAL_EMOJI = "ğŸ¾ğŸ¶ğŸ±ğŸ¾"
ANIMAL_BG_COLOR = "#FFF8E7"
ANIMAL_SIDEBAR_COLOR = "#FFE4E1"
ANIMAL_HEADER = f"<h1 style='text-align:center; color:#FF69B4;'>{ANIMAL_EMOJI} PET-LAW ì±—ë´‡ {ANIMAL_EMOJI}</h1>"
ANIMAL_SUBTITLE = "<p style='text-align:center; color:#A0522D;'>ì•ˆë…•í•˜ì„¸ìš”! ê·€ì—¬ìš´ ë™ë¬¼ ì¹œêµ¬ë“¤ì˜ ë²•ë¥ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” PET-LAW ì±—ë´‡ì´ì—ìš”!<br>ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¾</p>"

# ë°°ê²½ìƒ‰ ìŠ¤íƒ€ì¼ ì ìš©
st.markdown(f"""
    <style>
        .stApp {{
            background-color: {ANIMAL_BG_COLOR};
        }}
        section[data-testid="stSidebar"] {{
            background-color: {ANIMAL_SIDEBAR_COLOR};
        }}
    </style>
""", unsafe_allow_html=True)

# ìƒë‹¨ì— ë™ë¬¼ ì´ëª¨ì§€ì™€ íƒ€ì´í‹€
st.markdown(ANIMAL_HEADER, unsafe_allow_html=True)
st.markdown(ANIMAL_SUBTITLE, unsafe_allow_html=True)

# OpenAI API Key ì…ë ¥
api_key = st.sidebar.text_input("Enter your OpenAI API Key", type="password")
save_button = st.sidebar.button("Save Key")
api_key_valid = False
if save_button and len(api_key)>10:
    os.environ["OPENAI_API_KEY"] = api_key
    st.sidebar.success("API Key saved successfully! ğŸ¾")
    st.session_state["api_key_saved"] = True
if "api_key_saved" in st.session_state and st.session_state["api_key_saved"]:
    api_key_valid = True

# pet_doc.pdf ìë™ ë¡œë“œ (API í‚¤ê°€ ìˆì„ ë•Œë§Œ)
PET_DOC_PATH = os.path.join(os.path.dirname(__file__), "pet_doc.pdf")
vectorstore, raw_text = None, None
if api_key_valid:
    if not os.path.exists(PET_DOC_PATH):
        st.error("pet_doc.pdf íŒŒì¼ì´ í˜„ì¬ í´ë”ì— ì—†ìŠµë‹ˆë‹¤! ğŸ¾")
        st.stop()
    vectorstore, raw_text = process_uploaded_file(PET_DOC_PATH)
    if vectorstore:
        st.session_state['vectorstore'] = vectorstore
        st.session_state['raw_text'] = raw_text

# chatbot greatings
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ê·€ì—¬ìš´ ë™ë¬¼ ì¹œêµ¬ë“¤ì˜ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ëŠ” í«ë‹¥ ì±—ë´‡ì´ì—ìš”! ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ¶ğŸ±"
        )
    ]

# conversation history print 
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)
    
# API í‚¤ ì•ˆë‚´ ë° ì…ë ¥ì°½ í™œì„±í™” ì œì–´
if not api_key_valid:
    st.info("ì±—ë´‡ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ê³  ì €ì¥ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”! ğŸ¾")
    st.chat_input("API í‚¤ë¥¼ ì…ë ¥í•´ì•¼ ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!", disabled=True)
else:
    prompt = st.chat_input("'ìš”ì•½' ë˜ëŠ” ë™ë¬¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•´ë³´ì„¸ìš”! ğŸ¾")
    if prompt:
        st.session_state.messages.append(ChatMessage(role="user", content=prompt))
        st.chat_message("user").write(prompt)

        with st.chat_message("assistant"):
            stream_handler = StreamHandler(st.empty())
            try:
                if prompt == "ìš”ì•½":
                    response = generate_summarize(stream_handler)
                else:
                    response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)
            except Exception as e:
                response = f"âš ï¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”: {e}"
            st.session_state["messages"].append(
                ChatMessage(role="assistant", content=response)
            )
